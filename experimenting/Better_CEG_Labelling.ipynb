{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345a2df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Envelope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mEnvelope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getEnvelopeResult, generate_counter_example\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModelCalls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_data, make_batch, evaluate_model, update_model\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m makeDir, write_to_csv, copyfiles, copyFile, readConfigurations\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Envelope'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "import importlib\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from Envelope import getEnvelopeResult, generate_counter_example\n",
    "from ModelCalls import generate_data, make_batch, evaluate_model, update_model\n",
    "from Utils import makeDir, write_to_csv, copyfiles, copyFile, readConfigurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeEnvelopeResutlsToFile(print_string, error, no_cg, data_size, avg_violation, max_violation, log, print_count):\n",
    "    print('%d,%s envelope, %.4f'%(print_count, print_string, error), file=log)\n",
    "    print('%d,%s cgs/total points, %d ,%d'%(print_count, print_string, no_cg, data_size), file=log)\n",
    "    print('%d,%s average violation, %.4f'%(print_count, print_string, avg_violation), file=log)\n",
    "    print('%d,%s maximum violation, %.4f'%(print_count, print_string, max_violation), file=log)\n",
    "\n",
    "def collectEnvelopeMetric(NN_model, data, labels, log, monotonic_index, fold, direction, print_string, counter_example_generator, print_count):\n",
    "    print('%d,%s, %.4f'%(print_count, print_string, evaluate_model(NN_model, data, labels, getConfigurations(), evaluate)), file=log)\n",
    "    if getConfigurations()['scalability']:\n",
    "        error, no_cg, avg_violation, max_violation, env_metrics_dict,prediction_metrics_dict = getEnvelopeResult(data, labels, NN_model, monotonic_index,fold, getConfigurations()[\"is_parallel\"], counter_example_generator, output, getConfigurations(), direction)\n",
    "        print('%d,%s envelope metrics, %s'%(print_count, print_string+\" \"+direction, str(env_metrics_dict)), file=log)\n",
    "        print('%d,%s prediction metrics, %s'%(print_count, print_string+\" \"+direction,str(prediction_metrics_dict)), file=log)\n",
    "    else:\n",
    "        error, no_cg, avg_violation, max_violation = getEnvelopeResult(data, labels, NN_model, monotonic_index,fold, getConfigurations()[\"is_parallel\"], counter_example_generator, output, getConfigurations(), direction)\n",
    "    writeEnvelopeResutlsToFile(print_string+\" \"+direction, error, no_cg, len(data), avg_violation, max_violation, log, print_count)\n",
    "    return no_cg\n",
    "\n",
    "def get_counter_example_u_l(data_point, monotonic_index, data_index, f_x, fold):\n",
    "    counter_example_upper,elapsed_time_u,vio_u,ind_u = generate_counter_example(getConfigurations(), counter_example_generator_upper, data_point.copy(), monotonic_index, data_index, f_x, fold)\n",
    "\n",
    "    counter_example_lower,elapsed_time_l,vio_l,ind_l = generate_counter_example(getConfigurations(), counter_example_generator_lower, data_point.copy(), monotonic_index, data_index, f_x, fold)\n",
    "\n",
    "    return counter_example_upper, vio_u, ind_u, counter_example_lower, vio_l,ind_l\n",
    "\n",
    "def get_monoticity_direction(monotonic_index):\n",
    "    index = getConfigurations()['monotonic_indices'].index(str(monotonic_index))\n",
    "    return getConfigurations()['monotonicity_directions'][index]\n",
    "\n",
    "\n",
    "def counter_example_pairs(NN_model, monotonic_index, column_names, train_data, train_labels, logging):\n",
    "    if getConfigurations()['is_parallel']:\n",
    "        try:\n",
    "            return counter_example_pairs_parallel(NN_model, monotonic_index, column_names, train_data, train_labels, logging)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Parallel processing for counter pairs failed, trying sequential\")\n",
    "            return counter_example_pairs_seq(NN_model, monotonic_index, column_names, train_data, train_labels, logging)\n",
    "    else:\n",
    "        return counter_example_pairs_seq(NN_model, monotonic_index, column_names, train_data, train_labels, logging)\n",
    "\n",
    "\n",
    "def counter_example_pairs_parallel(NN_model, monotonic_index, column_names, train_data, train_labels, logging):\n",
    "    counter_batch = []\n",
    "    batch_labels = []\n",
    "    violation = []\n",
    "    variable_size = train_data.shape[1]\n",
    "    weights_directory = getConfigurations()['weight_files']\n",
    "    layers = getConfigurations()['layers']\n",
    "    monotonicity_direction = get_monoticity_direction(monotonic_index)\n",
    "    min_max_list = getConfigurations()['min_max_values'][getConfigurations()['column_names'][monotonic_index]]\n",
    "    print_count = 0\n",
    "\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    num_cores = int(getConfigurations()['num_cores'])\n",
    "    print(num_cores)\n",
    "    print(len(train_data))\n",
    "    no_chunks = math.ceil(len(train_data)/num_cores)\n",
    "    print(no_chunks)\n",
    "    _data_chunks = np.array_split(train_data, no_chunks)\n",
    "    all_results = []\n",
    "    with Parallel(n_jobs=num_cores) as parallel:\n",
    "        for chunk in _data_chunks:\n",
    "            results = parallel(delayed(counter_pair_generator)(point.copy(), monotonic_index, index, 0,weights_directory, layers, monotonicity_direction,min_max_list[0],min_max_list[1],getConfigurations()[\"column_types\"],\"\",logging,True,getConfigurations()['tmp_prefix']) for index,point in chunk.iterrows())\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    if len(all_results) != len(train_data):\n",
    "        print(\"Length mismatch\")\n",
    "        raise Exception(\"Length mismatch of parallel processing\")\n",
    "\n",
    "    for res in all_results:\n",
    "        counter_pair,elapsed_time, index = res\n",
    "        point = train_data.loc[index].values\n",
    "\n",
    "        if not counter_pair == None:\n",
    "            point_x = point.copy()\n",
    "            point_y = point.copy()\n",
    "            point_x[monotonic_index] = counter_pair[0]\n",
    "            point_y[monotonic_index] = counter_pair[1]\n",
    "            output_x = output(NN_model, point_x)[0][0]\n",
    "            output_y = output(NN_model, point_y)[0][0]\n",
    "            avg_f_x = (output_x + output_y)/2\n",
    "            violation.append(abs(1.0*(output_x-output_y)))\n",
    "            #     #We swap the labels\n",
    "            # print(\"X is \"+str(counter_pair[0]))\n",
    "            # print(\"X' is \"+str(counter_pair[1]))\n",
    "            # print(\"Y is \"+str(output_y))\n",
    "            # print(\"Y' is \"+str(output_x))\n",
    "\n",
    "            # counter_batch.append(point_x)\n",
    "            # counter_batch.append(point_y)\n",
    "            # batch_labels.append(output_y)\n",
    "            # batch_labels.append(output_x)\n",
    "            counter_batch.append(point_x)\n",
    "            counter_batch.append(point_y)\n",
    "            batch_labels.append(avg_f_x)\n",
    "            batch_labels.append(avg_f_x)\n",
    "    print(violation)\n",
    "    return counter_batch, batch_labels,violation\n",
    "    \n",
    "\n",
    "def counter_example_pairs_seq(NN_model, monotonic_index, column_names, train_data, train_labels, logging):\n",
    "    counter_batch = []\n",
    "    batch_labels = []\n",
    "    violation = []\n",
    "    variable_size = train_data.shape[1]\n",
    "    weights_directory = getConfigurations()['weight_files']\n",
    "    layers = getConfigurations()['layers']\n",
    "    monotonicity_direction = get_monoticity_direction(monotonic_index)\n",
    "    min_max_list = getConfigurations()['min_max_values'][getConfigurations()['column_names'][monotonic_index]]\n",
    "    print_count = 0\n",
    "    for index, point in train_data.iterrows():\n",
    "        print_count = print_count + 1\n",
    "        counter_pair,elapsed_time,ind = counter_pair_generator(point, monotonic_index, index, 0,weights_directory, layers, monotonicity_direction,min_max_list[0],min_max_list[1],getConfigurations()[\"column_types\"],\"\",logging,True,getConfigurations()['tmp_prefix'])\n",
    "        if not counter_pair == None:\n",
    "            point_x = point.copy()\n",
    "            point_y = point.copy()\n",
    "            point_x[monotonic_index] = counter_pair[0]\n",
    "            point_y[monotonic_index] = counter_pair[1]\n",
    "            output_x = output(NN_model, point_x)[0][0]\n",
    "            output_y = output(NN_model, point_y)[0][0]\n",
    "            violation.append(abs(1.0*(output_x-output_y)))\n",
    "            #     #We swap the labels\n",
    "            # print(\"X is \"+str(counter_pair[0]))\n",
    "            # print(\"X' is \"+str(counter_pair[1]))\n",
    "            # print(\"Y is \"+str(output_y))\n",
    "            # print(\"Y' is \"+str(output_x))\n",
    "\n",
    "            counter_batch.append(point_x)\n",
    "            counter_batch.append(point_y)\n",
    "            batch_labels.append(output_y)\n",
    "            batch_labels.append(output_x)\n",
    "\n",
    "    return counter_batch, batch_labels,violation\n",
    "    # counter_pair = counter_pair_generator(variable_size, monotonic_index, MIP_model, data_index,fold,weights_directory,layers,monotonicity_direction)\n",
    "    \n",
    "    # if counter_pair[0] == None:\n",
    "    # return None, None\n",
    "    # else: \n",
    "    #     counter_batch.append(counter_pair[0])\n",
    "    #     counter_batch.append(counter_pair[1])\n",
    "    #     counter_score_0 = output(NN_model, counter_pair[0])[0]\n",
    "    #     counter_score_1 = output(NN_model, counter_pair[1])[0]\n",
    "    #     #We swap the labels\n",
    "    #     batch_labels.append(counter_score_1)\n",
    "    #     batch_labels.append(counter_score_0)\n",
    "    #     counter_batch = pd.DataFrame(counter_batch, columns= column_names)\n",
    "    #     batch_labels = pd.DataFrame(batch_labels)\n",
    "    #     original_train = train_data\n",
    "    #     original_label = train_labels\n",
    "    #     test_batch = original_train.append(counter_batch, ignore_index = True) \n",
    "    #     test_batch_label = original_label.append(batch_labels, ignore_index = True)\n",
    "    #     print('Mean Squared Error after pair training')\n",
    "    #     NN_model,  MIP_model = update_model(test_batch, test_batch_label, NN_model, MIP_model,fold)\n",
    "    #     return NN_model,  MIP_model   \n",
    "\n",
    "\n",
    "def writeTimeToFile(time,fold,text=\"\"):\n",
    "    directory = getConfigurations()['log_files']\n",
    "    f = open(directory+\"timetaken.txt\", \"a\")\n",
    "    if not text or text==\"\":\n",
    "        f.write(\"Time taken for fold \"+str(fold)+\" is : \"+str(time) +\"\\n\")\n",
    "    else:\n",
    "        f.write(text+\" is : \"+str(time) +\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "def setupfolders():\n",
    "    run_data_dir = getConfigurations()['run_data_path']\n",
    "    log_files_path = run_data_dir + \"logs/\"\n",
    "    makeDir(log_files_path)\n",
    "    setConfigurations('log_files',log_files_path)\n",
    "\n",
    "    plot_files_path = run_data_dir + \"plots/\"\n",
    "    makeDir(plot_files_path)\n",
    "    setConfigurations('plot_files',plot_files_path)\n",
    "\n",
    "    weight_files_path = run_data_dir + \"folds/\"\n",
    "    makeDir(weight_files_path)\n",
    "    setConfigurations('weight_files',weight_files_path)\n",
    "\n",
    "def setConfigurations (key,value):\n",
    "    global configurations\n",
    "    configurations[key] = value\n",
    "\n",
    "def getConfigurations ():\n",
    "    global configurations\n",
    "    return configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9f43f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readConfigurations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Setting up the configurations ##\u001b[39;00m\n\u001b[1;32m      3\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../configurations/auto-mpg.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m configurations \u001b[38;5;241m=\u001b[39m \u001b[43mreadConfigurations\u001b[49m(configuration_file)\n\u001b[1;32m      5\u001b[0m solver_times \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#--- import the nn model --------\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'readConfigurations' is not defined"
     ]
    }
   ],
   "source": [
    "## Setting up the configurations ##\n",
    "\n",
    "configuration_file = \"../configurations/auto-mpg.txt\"\n",
    "configurations = readConfigurations(configuration_file)\n",
    "solver_times = []\n",
    "\n",
    "#--- import the nn model --------\n",
    "sys.path.append('./src/Models')\n",
    "output = importlib.__import__(getConfigurations()['model']).output\n",
    "evaluate = importlib.__import__(getConfigurations()['model']).evaluate\n",
    "make_data = importlib.__import__(getConfigurations()['model']).make_data\n",
    "update_batch = importlib.__import__(getConfigurations()['model']).update_batch\n",
    "\n",
    "#----- import solver functions --------\n",
    "counter_example_generator_upper = importlib.__import__(getConfigurations()['solver']).counter_example_generator_upper_env\n",
    "counter_example_generator_lower = importlib.__import__(getConfigurations()['solver']).counter_example_generator_lower_env\n",
    "\n",
    "counter_pair_generator = importlib.__import__(getConfigurations()['solver']).counter_pair_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d6576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "def verification(data_path, log_file, n_folds, monotonic_indices):\n",
    "    global train_data\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=getConfigurations()['run_data_path']+\"app.log\", filemode='w', format='%(name)s - %(levelname)s - %(message)s',level = logging.DEBUG)\n",
    "    logger = logging.getLogger('nnverification')\n",
    "\n",
    "    global solver_times\n",
    "    logging.info(\"Generating Data for train and test\")\n",
    "    \n",
    "    train_all, train_labels_all, test_all, test_labels_all,min_max_dict = generate_data(make_data, getConfigurations())\n",
    "\n",
    "    setConfigurations('min_max_values',min_max_dict)\n",
    "\n",
    "    column_names: List[str] = getConfigurations()['column_names']\n",
    "\n",
    "    fold = 0\n",
    "    initialModel = getConfigurations()['fold_data_dir']\n",
    "    copyFile(initialModel, getConfigurations()['weight_files'])\n",
    "\n",
    "    monotonic_index = monotonic_indices\n",
    "    start_time = time.time()\n",
    "    train_data = train_all[fold]\n",
    "    train_labels = train_labels_all[fold]\n",
    "    test_data = test_all[fold]\n",
    "    test_labels = test_labels_all[fold]\n",
    "    logging.debug(\"Mean Squared Error after initial training: \\n\")\n",
    "    model_file = configurations['model_dir'] + 'model.h5'\n",
    "    NN_model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    logging.debug(\"The train error is \")\n",
    "    logging.debug(evaluate_model(NN_model,train_data,train_labels,getConfigurations(), evaluate))\n",
    "    print(\"The train error is \")\n",
    "    print(evaluate_model(NN_model,train_data,train_labels,getConfigurations(), evaluate))\n",
    "    logging.debug(\"The test error is \")\n",
    "    logging.debug(evaluate_model(NN_model,test_data,test_labels,getConfigurations(),evaluate))\n",
    "    print(\"The test error is\")\n",
    "    print(evaluate_model(NN_model,test_data,test_labels,getConfigurations(),evaluate))\n",
    "\n",
    "    batches = make_batch(train_data, train_labels, getConfigurations())\n",
    "    batch_index = 0\n",
    "    temp_batch_count =0\n",
    "    plot_test_data =[]\n",
    "    plot_test_label=0\n",
    "    for index in batches[batch_index]:\n",
    "        plot_test_data = train_data.iloc[index].values\n",
    "        plot_test_label = train_labels.iloc[index]\n",
    "\n",
    "    with open(log_file+'log.txt',\"a\") as log:\n",
    "        # Stats with initial model:\n",
    "        NN_model.save(getConfigurations()['weight_files']+\"model_0.h5\")\n",
    "        isRetrainWCG = True\n",
    "        print_count = 0\n",
    "\n",
    "        print(\"Getting counter examples in test data...\")\n",
    "\n",
    "        print(\"Upper cegs..\")\n",
    "        start_env_time = time.time()\n",
    "        no_cg_upper = 61\n",
    "#         no_cg_upper = collectEnvelopeMetric(NN_model, test_data, test_labels, log, monotonic_index, fold, \"upper\", \"test\",counter_example_generator_upper,print_count)\n",
    "        elapse_env_time = time.time() - start_env_time\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(elapse_env_time)),fold,\"Test Upper Envelope time : \")\n",
    "\n",
    "        print(\"Lower cegs..\")\n",
    "        start_env_time = time.time()\n",
    "        no_cg_lower = 62\n",
    "#         no_cg_lower = collectEnvelopeMetric(NN_model, test_data, test_labels, log, monotonic_index, fold, \"lower\", \"test\",counter_example_generator_lower,print_count)\n",
    "        elapse_env_time = time.time() - start_env_time\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(elapse_env_time)),fold,\"Test Lower Envelope time : \")\n",
    "\n",
    "        isNoViolationTest = False\n",
    "        no_cg = no_cg_upper + no_cg_lower\n",
    "\n",
    "        print(\"Test Counter egs: \" , no_cg, \"(\", no_cg_upper, no_cg_lower, \")\")\n",
    "\n",
    "        if no_cg == 0:\n",
    "            isNoViolationTest = True\n",
    "\n",
    "        print(\"Getting counter examples in train data...\")\n",
    "\n",
    "        print(\"Upper cegs..\")\n",
    "        start_env_time = time.time()\n",
    "        no_cg_upper = 161\n",
    "#         no_cg_upper = collectEnvelopeMetric(NN_model, train_data, train_labels, log, monotonic_index, fold, \"upper\", \"train\", counter_example_generator_upper,print_count)\n",
    "        elapse_env_time = time.time() - start_env_time\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(elapse_env_time)),fold,\"Train Upper Envelope time : \")\n",
    "\n",
    "        print(\"Lower cegs..\")\n",
    "        start_env_time = time.time()\n",
    "        no_cg_lower = 162\n",
    "#         no_cg_lower = collectEnvelopeMetric(NN_model, train_data, train_labels, log, monotonic_index, fold, \"lower\", \"train\", counter_example_generator_lower,print_count)\n",
    "        elapse_env_time = time.time() - start_env_time\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(elapse_env_time)),fold,\"Train Lower Envelope time : \")\n",
    "\n",
    "        no_cg = no_cg_upper + no_cg_lower\n",
    "\n",
    "        isNoViolationTrain = False\n",
    "\n",
    "        if no_cg == 0:\n",
    "            isNoViolationTrain = True\n",
    "            isRetrainWCG = False\n",
    "\n",
    "        print(\"Train Counter egs: \" , no_cg, \"(\", no_cg_upper, no_cg_lower, \")\")\n",
    "\n",
    "        setConfigurations(\"scalability\", False)\n",
    "        if getConfigurations()['counter_example_type'] == \"cg\" and getConfigurations()['retrain_model']:\n",
    "            logging.debug('Counter Example Learning')\n",
    "            print('Counter Example Learning Starts ->')\n",
    "            # print(getConfigurations()['number_of_epochs'])\n",
    "            # return\n",
    "            for epoch in range(0,getConfigurations()['number_of_epochs']):\n",
    "                if not isRetrainWCG:\n",
    "                    logging.debug('No violations in train and test!')\n",
    "                    print('No violations in train and test!')\n",
    "                    NN_model = update_model(train_data, train_labels, NN_model,getConfigurations(),update_batch)\n",
    "                    isRetrainWCG = True\n",
    "                    # break\n",
    "                logging.debug('Starting epoch: %d'%(epoch))\n",
    "                print('Starting epoch: %d'%(epoch))\n",
    "\n",
    "                temp_batch_count = 0\n",
    "                number_counter_unsat = 0\n",
    "                \n",
    "                for batch in batches: \n",
    "                    counter_example_count = 0\n",
    "                    temp_batch_count = temp_batch_count+1\n",
    "\n",
    "                    logging.debug('Progress... batch/batches:%d/%d'%(temp_batch_count,len(batches)))\n",
    "                    print('At batch/batches:%d/%d'%(temp_batch_count,len(batches)), \"Epoch: \", epoch)\n",
    "\n",
    "                    counter_batch = []\n",
    "                    batch_labels = []\n",
    "                    count = 0\n",
    "                    num_cores = int(getConfigurations()['num_cores'])\n",
    "                    all_results = []\n",
    "\n",
    "                    print(\"Finding cgs for the batch...\")\n",
    "                    # Get both upper and lower counter eg for all points in this batch\n",
    "                    with Parallel(n_jobs=num_cores) as parallel:\n",
    "                        results = parallel(delayed(get_counter_example_u_l)(train_data.iloc[data_index].values, monotonic_index, data_index, output(NN_model, train_data.iloc[data_index].values)[0][0], fold) for data_index in batch)\n",
    "                        all_results.extend(results)\n",
    "\n",
    "                    # print(all_results[0])\n",
    "                    # print(len(all_results))\n",
    "\n",
    "                    # Process the counter examples!!\n",
    "                    for res in results:\n",
    "                        try:\n",
    "                            count = count+1\n",
    "                            counter_example_upper, vio_u, ind_u, counter_example_lower, vio_l,ind_l = res\n",
    "                            if ind_u != ind_l:\n",
    "                                print(\"The indices dont match\")\n",
    "                                sys.exit(0)\n",
    "                            data_point = train_data.iloc[ind_u].values\n",
    "                            logging.debug('CounterExample Progress... count/batchsize:%d/%d'%(count,len(batch)))\n",
    "                            print('CounterExample Progress... count/batchsize:%d/%d'%(count,len(batch)))\n",
    "                            \n",
    "                            print(min_max_dict[\"Displacement\"])\n",
    "                            data_point_copy = data_point\n",
    "#                             data_point_copy[1] = min_max_dict[\"Displacement\"][0]\n",
    "\n",
    "                            print(\"Data point:\")\n",
    "                            print(data_point)\n",
    "                            \n",
    "                            print(\"Data point copy:\")\n",
    "                            print(data_point_copy)\n",
    "                            \n",
    "                            print(\"Upper CEG:\")\n",
    "                            print(counter_example_upper)\n",
    "                            print(vio_u)\n",
    "                            print(ind_u)\n",
    "                            \n",
    "                            print(\"Lower CEG:\")\n",
    "                            print(counter_example_lower)\n",
    "                            print(vio_l)\n",
    "                            print(ind_l)\n",
    "                            \n",
    "\n",
    "                            counter_examples = []\n",
    "\n",
    "                            counter_examples.append(counter_example_upper)\n",
    "                            counter_examples.append(counter_example_lower)\n",
    "\n",
    "                            if getConfigurations()['is_classification']:\n",
    "                                f_x = NN_model.predict_classes(pd.DataFrame(data_point).transpose())[0][0]\n",
    "                                avg_f_x = f_x\n",
    "                            else:\n",
    "                                f_x = output(NN_model, data_point)[0][0]\n",
    "                                print(\"Orig: \", f_x)\n",
    "                                f_x_cgs = []\n",
    "                                f_x_cgs.append(f_x)\n",
    "                                for counter_example in counter_examples:\n",
    "                                    if counter_example is not None:\n",
    "                                        f_x_cgs.append(output(NN_model, counter_example)[0][0])\n",
    "                                avg_f_x = 1.0*sum(f_x_cgs)/len(f_x_cgs)\n",
    "\n",
    "                            counter_batch.append(data_point)\n",
    "                            batch_labels.append(avg_f_x)\n",
    "\n",
    "                            for counter_example in counter_examples:\n",
    "                                if counter_example is None:\n",
    "                                    number_counter_unsat = number_counter_unsat + 1\n",
    "                                else:\n",
    "                                    counter_example_count = counter_example_count+1\n",
    "                                    counter_batch.append(counter_example)\n",
    "                                    batch_labels.append(avg_f_x)\n",
    "                            \n",
    "                            print(counter_batch)\n",
    "                            print(f_x_cgs, sum(f_x_cgs))\n",
    "                            print(batch_labels)\n",
    "                            \n",
    "                        except:\n",
    "                            print(\"Exception while processing counterexample \" + sys.exc_info()[0])\n",
    "                            sys.exit(0)\n",
    "                        \n",
    "                        return\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    original_train = train_data\n",
    "                    original_label = train_labels\n",
    "\n",
    "                    if len(counter_batch) != len(batch_labels):\n",
    "                        print(\"Length of cg and label not equal\")\n",
    "                        sys.exit(0)\n",
    "\n",
    "                    print(len(original_train))\n",
    "                    if (len(counter_batch)>0):\n",
    "                        counter_batch = pd.DataFrame(counter_batch, columns= column_names)\n",
    "                        batch_labels = pd.DataFrame(batch_labels)\n",
    "\n",
    "                        # train_batch = original_train.append(counter_batch, ignore_index = True,sort=False)\n",
    "                        # train_batch_label = original_label.append(batch_labels, ignore_index = True)\n",
    "                        train_batch = pd.concat([original_train, counter_batch], ignore_index = True)\n",
    "                        train_batch_label = pd.concat([original_label, batch_labels], ignore_index = True)\n",
    "                        print(len(train_batch))\n",
    "                        \n",
    "\n",
    "                        # logging.debug('Mean Squared Error after batch %d/%d counterexample: '%((temp_batch_count,len(batches))))\n",
    "                        logging.debug(f'Mean Squared Error after batch {temp_batch_count}/{len(batches)} counterexample: ')\n",
    "                        NN_model = update_model(train_batch, train_batch_label, NN_model,getConfigurations(),update_batch)\n",
    "                    \n",
    "                    print(\"==========================\")\n",
    "                    return\n",
    "                    batch_index+=1\n",
    "\n",
    "                print(\"Here!\")\n",
    "\n",
    "                logging.debug(\"The model after epoch \"+str(epoch))\n",
    "                logging.debug(evaluate_model(NN_model, train_data,train_labels,getConfigurations(),evaluate))\n",
    "                # Save the model after each epoch\n",
    "                NN_model.save(getConfigurations()['weight_files']+\"model_\"+str(epoch+1)+\".h5\")\n",
    "                # Logging metrics after each epoch:\n",
    "                print_count = epoch+1\n",
    "\n",
    "                print(\"Finding updated number of CGS...\")\n",
    "                no_cg_upper = collectEnvelopeMetric(NN_model, test_data, test_labels, log, monotonic_index, fold, \"upper\", \"test\",counter_example_generator_upper, print_count)\n",
    "                no_cg_lower = collectEnvelopeMetric(NN_model, test_data, test_labels, log, monotonic_index, fold, \"lower\", \"test\",counter_example_generator_lower, print_count)\n",
    "                no_cg_upper_train = collectEnvelopeMetric(NN_model, train_data, train_labels, log, monotonic_index, fold, \"upper\", \"train\", counter_example_generator_upper, print_count)\n",
    "                no_cg_lower_train = collectEnvelopeMetric(NN_model, train_data, train_labels, log, monotonic_index, fold, \"lower\", \"train\", counter_example_generator_lower, print_count)\n",
    "\n",
    "                print(no_cg_upper, no_cg_lower, no_cg_upper_train, no_cg_lower_train)\n",
    "                no_cg = no_cg_upper + no_cg_lower\n",
    "\n",
    "                if no_cg == 0:\n",
    "                    isRetrainWCG = False\n",
    "\n",
    "        if not isRetrainWCG and getConfigurations()['retrain_model']:\n",
    "            #Jump here if both train and test errors are 0\n",
    "            print('No violation in test and train', file=log)\n",
    "            #plot graphs for a set of random points and check if it is indeed monotonic:\n",
    "            #Generate random points from test and train\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # print(\"total elapsetime is: \")\n",
    "        # print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)),fold)\n",
    "        average_time = 0.0\n",
    "        if len(solver_times) > 0:\n",
    "            average_time = (sum(solver_times) / len(solver_times))\n",
    "        writeTimeToFile(time.strftime(\"%H:%M:%S\", time.gmtime(average_time)),fold,\"Average Time taken to solve each query by solver \"+getConfigurations()['solver_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa06b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getConfigurations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dirname \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow())\n\u001b[1;32m      2\u001b[0m dir_col_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombination\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetConfigurations\u001b[49m()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonotonic_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     dir_col_string \u001b[38;5;241m=\u001b[39m dir_col_string \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m getConfigurations()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn_names\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mint\u001b[39m(m)]\n\u001b[1;32m      5\u001b[0m run_data_dir \u001b[38;5;241m=\u001b[39m getConfigurations()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_data_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m getConfigurations()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_benchmark\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m dir_col_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mdirname \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getConfigurations' is not defined"
     ]
    }
   ],
   "source": [
    "dirname = datetime.now().strftime('%Y%m%d')+ str(datetime.now())\n",
    "dir_col_string = \"Combination\"\n",
    "for m in getConfigurations()['monotonic_indices']:\n",
    "    dir_col_string = dir_col_string + \"+\" + getConfigurations()['column_names'][int(m)]\n",
    "run_data_dir = getConfigurations()['run_data_path']+ getConfigurations()['current_benchmark'] +\"/\" + dir_col_string +\"/\"+dirname +\"/\"\n",
    "makeDir(run_data_dir,True)\n",
    "setConfigurations('run_data_path',run_data_dir)\n",
    "setupfolders()\n",
    "configurations = getConfigurations()\n",
    "print(configurations['model_dir'])\n",
    "configurations['model_dir'] = \"../examples/Auto-MPG/\"\n",
    "configurations['fold_data_dir'] = \"../examples/Auto-MPG/\"\n",
    "verification(configurations['model_dir'], configurations['log_files'], configurations['n_folds'], getConfigurations()['monotonic_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62afd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fc4fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for index, row in train_data.iterrows():\n",
    "    rows.append(row.copy())\n",
    "    \n",
    "for i, r1 in enumerate(rows):\n",
    "    diction = getConfigurations()[\"min_max_values\"]\n",
    "    for k,v in diction.items():\n",
    "        r1[k] = float((r1[k] - v[0]))/(v[1] - v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "005b744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014679356057430677 27 29\n"
     ]
    }
   ],
   "source": [
    "l=[\"Displacement\"]\n",
    "diction = getConfigurations()[\"min_max_values\"]\n",
    "\n",
    "nearest = 10000\n",
    "nearest_i = -1\n",
    "nearest_j = -1\n",
    "for i, r1 in enumerate(rows):\n",
    "    for j, r2 in enumerate(rows): \n",
    "        fresh = 0\n",
    "        if i != j:\n",
    "            for k in diction.keys():\n",
    "                if k not in l:\n",
    "                    fresh = fresh + (r2[k] - r1[k]) * (r2[k] - r1[k])\n",
    "            if fresh < nearest:\n",
    "                nearest = fresh\n",
    "                nearest_i = i\n",
    "                nearest_j = j\n",
    "\n",
    "print(nearest, nearest_i, nearest_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c09ccc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cylinders       0.600000\n",
      "Displacement    0.470284\n",
      "Horsepower      0.293478\n",
      "Weight          0.507092\n",
      "Acceleration    0.446429\n",
      "Model Year      0.083333\n",
      "Origin          0.000000\n",
      "Name: 35, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(rows[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8fc5f47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cylinders       0.600000\n",
      "Displacement    0.423773\n",
      "Horsepower      0.293478\n",
      "Weight          0.494976\n",
      "Acceleration    0.446429\n",
      "Model Year      0.083333\n",
      "Origin          0.000000\n",
      "Name: 37, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(rows[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "33a6650b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "23.0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.iloc[35])\n",
    "print(train_labels.iloc[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df541da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      18.0\n",
       "1      15.0\n",
       "3      16.0\n",
       "4      17.0\n",
       "5      15.0\n",
       "       ... \n",
       "389    22.0\n",
       "391    36.0\n",
       "394    44.0\n",
       "395    32.0\n",
       "396    28.0\n",
       "Name: MPG, Length: 314, dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ae089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comet_venv",
   "language": "python",
   "name": "comet_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

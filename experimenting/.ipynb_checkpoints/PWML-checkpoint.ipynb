{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afef08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pointwise Monotonic Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a794082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from Utils import readConfigurations\n",
    "\n",
    "import json\n",
    "import pandas\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa62762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the configurations ##\n",
    "\n",
    "configuration_file = \"../configurations/auto-mpg.txt\"\n",
    "configurations = readConfigurations(configuration_file)\n",
    "\n",
    "## Some hacks for notebook\n",
    "configurations[\"model_dir\"] = \"../examples/Auto-MPG/\"\n",
    "# print(json.dumps(configurations, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0707b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   18. ]\n",
      " [  1.   15. ]\n",
      " [  2.   16. ]\n",
      " [  3.   17. ]\n",
      " [  4.   15. ]\n",
      " [  5.   14. ]\n",
      " [  6.   14. ]\n",
      " [  7.   15. ]\n",
      " [  8.   15. ]\n",
      " [  9.   14. ]\n",
      " [ 10.   14. ]\n",
      " [ 11.   24. ]\n",
      " [ 12.   22. ]\n",
      " [ 13.   26. ]\n",
      " [ 14.   25. ]\n",
      " [ 15.   24. ]\n",
      " [ 16.   25. ]\n",
      " [ 17.   26. ]\n",
      " [ 18.   10. ]\n",
      " [ 19.   10. ]\n",
      " [ 20.   11. ]\n",
      " [ 21.    9. ]\n",
      " [ 22.   27. ]\n",
      " [ 23.   28. ]\n",
      " [ 24.   25. ]\n",
      " [ 25.   19. ]\n",
      " [ 26.   16. ]\n",
      " [ 27.   17. ]\n",
      " [ 28.   19. ]\n",
      " [ 29.   18. ]\n",
      " [ 30.   14. ]\n",
      " [ 31.   14. ]\n",
      " [ 32.   14. ]\n",
      " [ 33.   14. ]\n",
      " [ 34.   12. ]\n",
      " [ 35.   13. ]\n",
      " [ 36.   18. ]\n",
      " [ 37.   23. ]\n",
      " [ 38.   28. ]\n",
      " [ 39.   30. ]\n",
      " [ 40.   30. ]\n",
      " [ 41.   31. ]\n",
      " [ 42.   35. ]\n",
      " [ 43.   27. ]\n",
      " [ 44.   26. ]\n",
      " [ 45.   24. ]\n",
      " [ 46.   25. ]\n",
      " [ 47.   20. ]\n",
      " [ 48.   21. ]\n",
      " [ 49.   13. ]\n",
      " [ 50.   17. ]\n",
      " [ 51.   11. ]\n",
      " [ 52.   13. ]\n",
      " [ 53.   12. ]\n",
      " [ 54.   13. ]\n",
      " [ 55.   19. ]\n",
      " [ 56.   15. ]\n",
      " [ 57.   13. ]\n",
      " [ 58.   13. ]\n",
      " [ 59.   22. ]\n",
      " [ 60.   21. ]\n",
      " [ 61.   26. ]\n",
      " [ 62.   22. ]\n",
      " [ 63.   28. ]\n",
      " [ 64.   23. ]\n",
      " [ 65.   28. ]\n",
      " [ 66.   13. ]\n",
      " [ 67.   14. ]\n",
      " [ 68.   13. ]\n",
      " [ 69.   14. ]\n",
      " [ 70.   15. ]\n",
      " [ 71.   12. ]\n",
      " [ 72.   13. ]\n",
      " [ 73.   13. ]\n",
      " [ 74.   14. ]\n",
      " [ 75.   12. ]\n",
      " [ 76.   13. ]\n",
      " [ 77.   18. ]\n",
      " [ 78.   16. ]\n",
      " [ 79.   18. ]\n",
      " [ 80.   23. ]\n",
      " [ 81.   26. ]\n",
      " [ 82.   11. ]\n",
      " [ 83.   12. ]\n",
      " [ 84.   13. ]\n",
      " [ 85.   12. ]\n",
      " [ 86.   18. ]\n",
      " [ 87.   20. ]\n",
      " [ 88.   22. ]\n",
      " [ 89.   18. ]\n",
      " [ 90.   19. ]\n",
      " [ 91.   26. ]\n",
      " [ 92.   16. ]\n",
      " [ 93.   29. ]\n",
      " [ 94.   20. ]\n",
      " [ 95.   19. ]\n",
      " [ 96.   15. ]\n",
      " [ 97.   24. ]\n",
      " [ 98.   20. ]\n",
      " [ 99.   20. ]\n",
      " [100.   19. ]\n",
      " [101.   15. ]\n",
      " [102.   31. ]\n",
      " [103.   26. ]\n",
      " [104.   32. ]\n",
      " [105.   25. ]\n",
      " [106.   16. ]\n",
      " [107.   16. ]\n",
      " [108.   18. ]\n",
      " [109.   16. ]\n",
      " [110.   13. ]\n",
      " [111.   14. ]\n",
      " [112.   14. ]\n",
      " [113.   29. ]\n",
      " [114.   26. ]\n",
      " [115.   26. ]\n",
      " [116.   31. ]\n",
      " [117.   28. ]\n",
      " [118.   24. ]\n",
      " [119.   26. ]\n",
      " [120.   24. ]\n",
      " [121.   26. ]\n",
      " [122.   31. ]\n",
      " [123.   19. ]\n",
      " [124.   18. ]\n",
      " [125.   15. ]\n",
      " [126.   15. ]\n",
      " [127.   16. ]\n",
      " [128.   15. ]\n",
      " [129.   16. ]\n",
      " [130.   17. ]\n",
      " [131.   16. ]\n",
      " [132.   15. ]\n",
      " [133.   18. ]\n",
      " [134.   21. ]\n",
      " [135.   20. ]\n",
      " [136.   29. ]\n",
      " [137.   23. ]\n",
      " [138.   20. ]\n",
      " [139.   23. ]\n",
      " [140.   24. ]\n",
      " [141.   25. ]\n",
      " [142.   18. ]\n",
      " [143.   29. ]\n",
      " [144.   23. ]\n",
      " [145.   22. ]\n",
      " [146.   33. ]\n",
      " [147.   28. ]\n",
      " [148.   25. ]\n",
      " [149.   25. ]\n",
      " [150.   26. ]\n",
      " [151.   27. ]\n",
      " [152.   17.5]\n",
      " [153.   16. ]\n",
      " [154.   15.5]\n",
      " [155.   14.5]\n",
      " [156.   22. ]\n",
      " [157.   22. ]\n",
      " [158.   24. ]\n",
      " [159.   22.5]\n",
      " [160.   29. ]\n",
      " [161.   24.5]\n",
      " [162.   33. ]\n",
      " [163.   20. ]\n",
      " [164.   18. ]\n",
      " [165.   18.5]\n",
      " [166.   17.5]\n",
      " [167.   29.5]\n",
      " [168.   32. ]\n",
      " [169.   28. ]\n",
      " [170.   26.5]\n",
      " [171.   13. ]\n",
      " [172.   19. ]\n",
      " [173.   19. ]\n",
      " [174.   16.5]\n",
      " [175.   16.5]\n",
      " [176.   13. ]\n",
      " [177.   13. ]\n",
      " [178.   31.5]\n",
      " [179.   30. ]\n",
      " [180.   36. ]\n",
      " [181.   33.5]\n",
      " [182.   17.5]\n",
      " [183.   15.5]\n",
      " [184.   15. ]\n",
      " [185.   17.5]\n",
      " [186.   20.5]\n",
      " [187.   19. ]\n",
      " [188.   18.5]\n",
      " [189.   16. ]\n",
      " [190.   15.5]\n",
      " [191.   15.5]\n",
      " [192.   29. ]\n",
      " [193.   24.5]\n",
      " [194.   26. ]\n",
      " [195.   25.5]\n",
      " [196.   30.5]\n",
      " [197.   33.5]\n",
      " [198.   30.5]\n",
      " [199.   22. ]\n",
      " [200.   21.5]\n",
      " [201.   21.5]\n",
      " [202.   43.1]\n",
      " [203.   32.8]\n",
      " [204.   39.4]\n",
      " [205.   36.1]\n",
      " [206.   19.4]\n",
      " [207.   20.2]\n",
      " [208.   20.5]\n",
      " [209.   20.2]\n",
      " [210.   25.1]\n",
      " [211.   20.5]\n",
      " [212.   19.4]\n",
      " [213.   20.8]\n",
      " [214.   18.6]\n",
      " [215.   18.1]\n",
      " [216.   17.7]\n",
      " [217.   18.1]\n",
      " [218.   17.5]\n",
      " [219.   30. ]\n",
      " [220.   27.5]\n",
      " [221.   27.2]\n",
      " [222.   30.9]\n",
      " [223.   21.1]\n",
      " [224.   23.2]\n",
      " [225.   20.3]\n",
      " [226.   17. ]\n",
      " [227.   21.6]\n",
      " [228.   16.2]\n",
      " [229.   31.5]\n",
      " [230.   29.5]\n",
      " [231.   21.5]\n",
      " [232.   19.8]\n",
      " [233.   22.3]\n",
      " [234.   20.6]\n",
      " [235.   17.6]\n",
      " [236.   16.5]\n",
      " [237.   18.2]\n",
      " [238.   15.5]\n",
      " [239.   19.2]\n",
      " [240.   18.5]\n",
      " [241.   35.7]\n",
      " [242.   27.4]\n",
      " [243.   23. ]\n",
      " [244.   27.2]\n",
      " [245.   23.9]\n",
      " [246.   34.2]\n",
      " [247.   34.5]\n",
      " [248.   31.8]\n",
      " [249.   37.3]\n",
      " [250.   28.4]\n",
      " [251.   28.8]\n",
      " [252.   26.8]\n",
      " [253.   33.5]\n",
      " [254.   38.1]\n",
      " [255.   32.1]\n",
      " [256.   28. ]\n",
      " [257.   26.4]\n",
      " [258.   24.3]\n",
      " [259.   19.1]\n",
      " [260.   34.3]\n",
      " [261.   31.3]\n",
      " [262.   37. ]\n",
      " [263.   32.2]\n",
      " [264.   46.6]\n",
      " [265.   27.9]\n",
      " [266.   44.3]\n",
      " [267.   43.4]\n",
      " [268.   36.4]\n",
      " [269.   30. ]\n",
      " [270.   44.6]\n",
      " [271.   33.8]\n",
      " [272.   23.7]\n",
      " [273.   32.4]\n",
      " [274.   26.6]\n",
      " [275.   30. ]\n",
      " [276.   39.1]\n",
      " [277.   35.1]\n",
      " [278.   37. ]\n",
      " [279.   37.7]\n",
      " [280.   34.1]\n",
      " [281.   34.7]\n",
      " [282.   34.4]\n",
      " [283.   29.9]\n",
      " [284.   33.7]\n",
      " [285.   31.6]\n",
      " [286.   28.1]\n",
      " [287.   30.7]\n",
      " [288.   24.2]\n",
      " [289.   22.4]\n",
      " [290.   26.6]\n",
      " [291.   20.2]\n",
      " [292.   17.6]\n",
      " [293.   28. ]\n",
      " [294.   27. ]\n",
      " [295.   34. ]\n",
      " [296.   31. ]\n",
      " [297.   29. ]\n",
      " [298.   27. ]\n",
      " [299.   24. ]\n",
      " [300.   36. ]\n",
      " [301.   37. ]\n",
      " [302.   38. ]\n",
      " [303.   36. ]\n",
      " [304.   36. ]\n",
      " [305.   32. ]\n",
      " [306.   38. ]\n",
      " [307.   25. ]\n",
      " [308.   26. ]\n",
      " [309.   22. ]\n",
      " [310.   36. ]\n",
      " [311.   44. ]\n",
      " [312.   32. ]\n",
      " [313.   28. ]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "## Getting the data ##\n",
    "\n",
    "from ModelCalls import generate_data, make_batch, evaluate_model, update_model\n",
    "sys.path.append('../src/Models')\n",
    "make_data = importlib.__import__(configurations['model']).make_data\n",
    "\n",
    "train_all, train_labels_all, test_all, test_labels_all, min_max_dict = generate_data(make_data, configurations)\n",
    "\n",
    "train_data = train_all[0]\n",
    "train_labels = train_labels_all[0]\n",
    "test_data = test_all[0]\n",
    "test_labels = test_labels_all[0]\n",
    "\n",
    "# train_labels['id'] = range(0, len(train_labels))\n",
    "# print(train_labels)\n",
    "# for x in train_labels:\n",
    "#     x = 1\n",
    "train_labels = np.asarray([([a,b]) for a,b in enumerate(train_labels)])\n",
    "test_labels = np.asarray([([a,b]) for a,b in enumerate(test_labels)])\n",
    "\n",
    "print(train_labels)\n",
    "print(type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55422b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-26 17:50:48.999732: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 22ms/step - loss: 54466.5117 - val_loss: 9246.2305\n",
      "Epoch 2/15\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 23096.2637 - val_loss: 6204.7769\n",
      "Epoch 3/15\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7031.4043 - val_loss: 11816.7080\n",
      "Epoch 4/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 5388.2788 - val_loss: 1270.5370\n",
      "Epoch 5/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 3195.3635 - val_loss: 464.9224\n",
      "Epoch 6/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2414.5325 - val_loss: 1235.9750\n",
      "Epoch 7/15\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1849.4840 - val_loss: 1695.0393\n",
      "Epoch 8/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1352.8494 - val_loss: 1329.0944\n",
      "Epoch 9/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1140.7952 - val_loss: 1143.5887\n",
      "Epoch 10/15\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 978.9892 - val_loss: 703.4276\n",
      "Epoch 11/15\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 779.2225 - val_loss: 497.7801\n",
      "Epoch 12/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 660.5366 - val_loss: 505.0123\n",
      "Epoch 13/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 529.7051 - val_loss: 215.8208\n",
      "Epoch 14/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 429.5092 - val_loss: 134.2100\n",
      "Epoch 15/15\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 391.6821 - val_loss: 171.1343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287626aa0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(7,)))\n",
    "model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "def custom_loss_envelope(model, x):\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)      \n",
    "        \n",
    "    def custom_loss(y_true, y_pred):        \n",
    "        \n",
    "        correct_y_true = tf.gather(y_true, [1], axis=1)\n",
    "        loss_PDE = tf.keras.losses.mean_squared_error(correct_y_true, y_pred)\n",
    "        \n",
    "        with tf.GradientTape() as t:\n",
    "            t.watch(x_tensor)\n",
    "            output = model(x_tensor)\n",
    "        \n",
    "        x_indices = tf.gather(y_true, [0], axis=1)\n",
    "        x_indices = tf.reshape(x_indices, [-1])\n",
    "#         tf.print(x_indices)\n",
    "        \n",
    "        DyDx = t.gradient(output, x_tensor)\n",
    "#         tf.print(DyDx)\n",
    "#         tf.print(x_indices)\n",
    "        \n",
    "#         temp = DyDx[x_indices]\n",
    "        temp = tf.map_fn(lambda x: DyDx[int(x)], x_indices)\n",
    "# #         tf.print(temp)\n",
    "        \n",
    "        main_row = tf.gather(temp, [1], axis=1)\n",
    "        main_row = tf.reshape(main_row, [-1])\n",
    "# #         tf.print(main_row)\n",
    "# #         tf.print(\"----JMD---\")\n",
    "        final = tf.map_fn(lambda x: 0.0 if x < 0.0 else x, main_row)\n",
    "#         tf.print(final)\n",
    "    \n",
    "        return loss_PDE + final\n",
    "    return custom_loss\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=custom_loss_envelope(model, train_data))\n",
    "model.fit(train_data, train_labels, epochs=3, batch_size=32, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7bfce63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170.01385905612668\n"
     ]
    }
   ],
   "source": [
    "def my_evaluate():\n",
    "    x_tensor = tf.convert_to_tensor(test_data, dtype=tf.float32)\n",
    "    output = model(x_tensor)\n",
    "    output = tf.reshape(output, [-1]).numpy()\n",
    "    labels = test_labels[:,1]\n",
    "    \n",
    "    xx = output - labels\n",
    "    \n",
    "    ans = 0\n",
    "    for n in xx:\n",
    "        ans += n*n\n",
    "    \n",
    "    print(ans/len(xx))\n",
    "\n",
    "my_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 => 42.05 45.77\n",
    "\n",
    "10 => 360, 575.477\n",
    "\n",
    "15 -> 82.86, 170.013\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e98185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b2a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87d2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf9f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634423e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa38599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a0298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f8e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406ddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69715cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2de4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebea9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bd2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50041b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129299fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fe3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6cacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fefaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419012d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3905be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2858ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d23ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52937130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(input_tensor, output_tensor):\n",
    "    return tf.keras.layers.Lambda( lambda z: tf.keras.backend.gradients( z[ 0 ], z[ 1 ] ), output_shape = [1] )( [ output_tensor, input_tensor ] )\n",
    "\n",
    "def custom_loss_wrapper(model, input_tensor):\n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        mse_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            print(input_tensor)\n",
    "            output = model(input_tensor)\n",
    "            print(output)\n",
    "        \n",
    "#         print(tape.gradient(output, input_tensor))\n",
    "            \n",
    "#         derivative_loss = tf.keras.losses.mean_squared_error(input_tensor, -grad(input_tensor, grad(input_tensor, output_tensor))[0])\n",
    "        return mse_loss\n",
    "\n",
    "    return custom_loss\n",
    "\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63577f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating own model\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(7,)))\n",
    "model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(12, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "\n",
    "# model.summary() \n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=custom_loss_wrapper(model, model.input))\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_split = 0.2, verbose = 0)\n",
    "\n",
    "# print(\"Hi\", model.input)\n",
    "\n",
    "# input_m = model.input\n",
    "# with tf.GradientTape() as tape:\n",
    "# #     tape.watch(input_m)\n",
    "#     output = model(input_m)\n",
    "#     print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "model.variables\n",
    "# inp = tf.Variable(np.random.normal(size=(1, 7)), dtype=tf.float32)\n",
    "\n",
    "# inp = tf.Variable(model.input)\n",
    "# with tf.GradientTape() as tape:\n",
    "#     preds = model(model.input)\n",
    "\n",
    "# grads = tape.gradient(preds, model.input)\n",
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac30096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b121c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comet_venv",
   "language": "python",
   "name": "comet_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
